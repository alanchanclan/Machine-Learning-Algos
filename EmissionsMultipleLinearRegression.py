import matplotlib.pyplot as plt
import pandas as pd
import pylab
import numpy as np

df = pd.read_csv("FuelConsumptionCo2.csv")

# Take a peek
df.head()

# Summarize
df.describe()

# Select the first nine rows for the following columns:
cdf = df[['ENGINESIZE', 'CYLINDERS', 'FUELCONSUMPTION_COMB', 'CO2EMISSIONS']]
cdf.head(9)

# Scatter plot between Emissions and Engine Size
plt.scatter(cdf.ENGINESIZE, cdf.CO2EMISSIONS, color='blue')
plt.xlabel('Engine Size')
plt.ylabel('Emissions')
plt.show()

# Random rows 80 train 20 test
msk = np.random.rand(len(df)) < 0.8
train = cdf[msk]
test = cdf[~msk]

# Plot the training data
plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS, color='blue')
plt.xlabel('Engine Size')
plt.ylabel('Emissions')
plt.show()

# Use scikit-learn to do MULTIPLE LINEAR REGRESSION
from sklearn import linear_model
regr = linear_model.LinearRegression()
x = np.asanyarray(train[['ENGINESIZE', 'CYLINDERS', 'FUELCONSUMPTION_COMB']])
y = np.asanyarray(train[['CO2EMISSIONS']])
regr.fit(x, y)
# The co-efficients
print("Coefficients: ", regr.coef_)

"""
Ordinary Least Squares (OLS)
OLS is a method for estimating the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by minimizing the sum of the squares of the differences between the target dependent variable and those predicted by the linear function. In other words, it tries to minimizes the sum of squared errors (SSE) or mean squared error (MSE) between the target variable (y) and our predicted output ( ð‘¦Ì‚  ) over all samples in the dataset.

OLS can find the best parameters using of the following methods:

- Solving the model parameters analytically using closed-form equations
- Using an optimization algorithm (Gradient Descent, Stochastic Gradient Descent, Newtonâ€™s Method, etc.)


Explained variance regression score:
If  ð‘¦Ì‚   is the estimated target output, y the corresponding (correct) target output, and Var is Variance, the square of the standard deviation, then the explained variance is estimated as follow:

ðšŽðš¡ðš™ðš•ðšŠðš’ðš—ðšŽðšðš…ðšŠðš›ðš’ðšŠðš—ðšŒðšŽ(ð‘¦,ð‘¦Ì‚ )=1âˆ’ð‘‰ð‘Žð‘Ÿ{ð‘¦âˆ’ð‘¦Ì‚ }/ð‘‰ð‘Žð‘Ÿ{ð‘¦} 
The best possible score is 1.0, lower values are worse.
"""

y_hat = regr.predict(test[['ENGINESIZE', 'CYLINDERS', 'FUELCONSUMPTION_COMB']]) # Uses the predictions predicted prior
x = np.asanyarray(test[['ENGINESIZE', 'CYLINDERS', 'FUELCONSUMPTION_COMB']])
y = np.asanyarray(test[['CO2EMISSIONS']])
print("Residual sum of squares: %.2f" % np.mean((y_hat - y) ** 2))

# Explained variance score
print("Variance score %.2f" % regr.score(x, y))
